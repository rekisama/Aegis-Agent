#!/usr/bin/env python3
"""
LLM-Driven Task Analysis Test
æµ‹è¯•å®Œå…¨ç”± LLM é©±åŠ¨çš„ä»»åŠ¡åˆ†æ
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(str(Path(__file__).parent))


async def test_llm_driven_analysis():
    """æµ‹è¯•å®Œå…¨ç”± LLM é©±åŠ¨çš„ä»»åŠ¡åˆ†æ"""
    
    print("ğŸ§ª LLM-Driven Task Analysis Test")
    print("=" * 50)
    print("è¿™ä¸ªæµ‹è¯•éªŒè¯ Agent æ˜¯å¦å®Œå…¨ç”± LLM è‡ªä¸»åˆ¤æ–­ä»»åŠ¡ç±»å‹")
    print("=" * 50)
    
    from python.agent.self_evolving_core import SelfEvolvingAgent
    from python.agent.core import AgentConfig
    
    config = AgentConfig(
        name="LLM-Driven Agent",
        model="deepseek-chat",
        temperature=0.7,
        max_tokens=4000,
        memory_enabled=True,
        hierarchical_enabled=True,
        tools_enabled=True
    )
    
    agent = SelfEvolvingAgent(config)
    
    print(f"ğŸ¤– Created agent: {agent.config.name}")
    
    # æµ‹è¯•å„ç§ä¸åŒç±»å‹çš„ä»»åŠ¡
    test_tasks = [
        "ä¸œäº¬ç°åœ¨å‡ ç‚¹äº†",
        "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·",
        "è®¡ç®— 123 * 456 çš„ç»“æœ",
        "ç¿»è¯‘ 'Hello World' ä¸ºä¸­æ–‡",
        "è·å–ç¾å…ƒå…‘äººæ°‘å¸çš„æ±‡ç‡",
        "åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼šä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ„‰å¿«",
        "ç”Ÿæˆä¸€ä¸ªéšæœºå¯†ç ",
        "æ£€æŸ¥è¿™ä¸ªé‚®ç®±åœ°å€æ˜¯å¦æœ‰æ•ˆï¼štest@example.com",
        "å¸®æˆ‘å†™ä¸€ä¸ª Python å‡½æ•°",
        "æŸ¥æ‰¾æœ€æ–°çš„ç§‘æŠ€æ–°é—»"
    ]
    
    for i, task in enumerate(test_tasks, 1):
        print(f"\nğŸ” Test {i}: {task}")
        print("-" * 50)
        
        try:
            # ç›´æ¥æµ‹è¯•ä»»åŠ¡åˆ†æåŠŸèƒ½
            analysis = await agent._analyze_task_for_tool_creation(task)
            
            print(f"ğŸ“Š Analysis Result:")
            print(f"   Should create tool: {analysis.get('should_create_tool', False)}")
            
            if analysis.get('should_create_tool', False):
                print(f"   Tool name: {analysis.get('tool_name', 'N/A')}")
                print(f"   Tool description: {analysis.get('tool_description', 'N/A')}")
                print(f"   Implementation approach: {analysis.get('implementation_approach', 'N/A')}")
                print(f"   Reasoning: {analysis.get('reasoning', 'N/A')}")
                print(f"   Existing tools analysis: {analysis.get('existing_tools_analysis', 'N/A')}")
                
                # æµ‹è¯•å·¥å…·åˆ›å»º
                tool_created = await agent._create_dynamic_tool_from_analysis(analysis)
                print(f"   Tool creation: {'âœ… Success' if tool_created else 'âŒ Failed'}")
            else:
                print(f"   Reasoning: {analysis.get('reasoning', 'No specialized tool needed')}")
                
        except Exception as e:
            print(f"âŒ Test failed: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nğŸ‰ LLM-Driven Analysis Test Completed!")
    print("=" * 50)


async def test_full_execution_with_llm_analysis():
    """æµ‹è¯•å®Œæ•´æ‰§è¡Œæµç¨‹ä¸­çš„ LLM åˆ†æ"""
    
    print("\nğŸš€ Full Execution with LLM Analysis Test")
    print("=" * 50)
    
    from python.agent.self_evolving_core import SelfEvolvingAgent
    from python.agent.core import AgentConfig
    
    config = AgentConfig(
        name="LLM-Driven Agent",
        model="deepseek-chat",
        temperature=0.7,
        max_tokens=4000,
        memory_enabled=True,
        hierarchical_enabled=True,
        tools_enabled=True
    )
    
    agent = SelfEvolvingAgent(config)
    
    test_tasks = [
        "ä¸œäº¬ç°åœ¨å‡ ç‚¹äº†",
        "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·",
        "è®¡ç®— 123 * 456 çš„ç»“æœ"
    ]
    
    for i, task in enumerate(test_tasks, 1):
        print(f"\nğŸ” Executing task {i}: {task}")
        print("-" * 50)
        
        try:
            # æ‰§è¡Œä»»åŠ¡
            result = await agent.execute_task(task)
            
            print(f"âœ… Task completed")
            print(f"ğŸ“Š Result: {result.get('result', 'No result')}")
            
            # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†åŠ¨æ€å·¥å…·
            from python.agent.dynamic_tool_creator import dynamic_tool_creator
            stats = dynamic_tool_creator.get_tool_statistics()
            tools = stats.get("tools", [])
            
            if tools:
                print(f"ğŸ› ï¸  Dynamic tools available: {len(tools)}")
                for tool in tools[-1:]:  # åªæ˜¾ç¤ºæœ€æ–°åˆ›å»ºçš„å·¥å…·
                    print(f"   â€¢ {tool['name']}: {tool['description']}")
            else:
                print("â„¹ï¸  No new dynamic tools created")
                
        except Exception as e:
            print(f"âŒ Execution failed: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nğŸ‰ Full Execution Test Completed!")
    print("=" * 50)


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        await test_llm_driven_analysis()
        await test_full_execution_with_llm_analysis()
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        logging.error(f"Test error: {e}")


if __name__ == "__main__":
    asyncio.run(main()) 